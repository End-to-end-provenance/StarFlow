#!/usr/bin/env python
'''
Routines for obtaining, generating, and processing meta data about data files,
file runs, and functions used in the data file generation process.

The way that the metadata scheme works is:

In the top level System Folder there is a directory ../System/MetaData that
contains a replica of the file structure outside of the System folder.
Each "real" path corresponds to a directory in the MetaData directory,
in which metadata about the path is stored,
e.g. '../Data/Dan_Data/NPR_Puzzle_Solutions', would correspond to
'../System/MetaData/Data/Dan_Data/NPR_Puzzle_Solutions'.
Metadata is similarly attached to functions in python modules referenced via their dot-paths.

Given a file path P, the associated path for the metadata is given by the
function metadatapath(P).  Given a python function dot path, the associated
path P for the metadata is given by the function opmetadatapath(P).

Inside a given path's metadata directory is a variety of associated metadata files.
There are three main kinds of associated things:
    -- data generated during the runtime that produced/was generated by the path.
    -- metadata attached by the human directly to the file.
    -- data used and generated by the system Graphical Browser.
    e.g. Linklists local to the path, and graphviz-generated graphs of these LinkLists.
    (See commends in System/SystemGraphOperations.py for information about this)

'''

import sys

try:
    #remove any existing versions of tabular
    sys.path.remove('/Users/jen/miniconda3/envs/python3.4/lib/python3.4/site-packages/tabular-0.1-py3.4.egg')
except:
    print("removed")

# use the python3 updated version in tabular that exists in the cloned directory of StarFlow
# change []/Users/jen/Desktop] to [your/path/to]StarFlow.
sys.path.append("/Users/jen/Desktop/StarFlow/")

import marshal
import os
import types
import hashlib
import traceback
import tabular as tb
import pickle as pickle

from starflow.utils import *
from starflow.storage import StoredDocstring
from starflow import static
import starflow.de as de
import imp

DE_MANAGER = de.DataEnvironmentManager()
WORKING_DE = DE_MANAGER.working_de

from datetime import datetime

def AttachMetaData(NewMetaData,FileName = '',OperationName='',Resources = None,
                   creates = WORKING_DE.relative_metadata_dir):
    '''
    Attach metadata to a file a given path.

    ARGUMENTS:
    --NewMetaData : a dictionary that will be pickled as metadata.
    --FileName = Path of the file to which the metadata should be attached.
    --OperationName = Python dot-path of the operati which the metadata should
    be attached.
    (Either File or OperationName should be specified, but not both.)

    Metadata attached through this process is put into the file
        MetaDataPath/AttachedMetaData.pickle.
    If this path exists at the time that an AttachMetaData command is run,
    the metadata dictionary in the file is updated with the NewMetaData dictionary. '''

    assert isinstance(NewMetaData,dict), 'The metadata to attach must be a dictionary.'

    if FileName != '':
        metapath = metadatapath(FileName)
    elif OperationName != '':
        metapath = opmetadatapath(OperationName)

    if not PathExists(metapath):
        MakeDirs(metapath)

    mdp = metapath + '/AttachedMetaData.pickle'
    if PathExists(mdp):
        try:
            ExistingMetaData = pickle.load(open(mdp,'rb'))
            if not isinstance(ExistingMetaData,dict):
                ExistingMetaData = {}
        except:
            ExistingMetaData = {}
    else:
        ExistingMetaData = {}

    ExistingMetaData.update(NewMetaData)

    F = open(mdp,'wb')
    pickle.dump(ExistingMetaData,F)
    F.close()

    ProcessMetaData(metapath,FileName if FileName else OperationName,extensions=['Attached'])

def MakeRuntimeMetaData(opname,Creates,OriginalTimes,OriginalDirInfo,RunOutput,ExitType,ExitStatus,Before,After,IsDifferent,TempSOIS):

    '''
    This is an internal usage function that attaches the results of
    metadata generated during the running of a system update.
    It is used primilary by the UpdateLinks function in ../System/Update.py

    Suppose the function F is run by the automatic updater.
    During this runtime, several kinds of data are produced:
    --the return output of the function F, and
    --information about the run, its Exit status, whether it changed any
        files it was meant to produce, etc...

    These pieces of runtime metadata are collected by the UpdateLink fnction
    during update, and then passed to MakeRuntimeMetaData function to write
    out the data in an appropriate format to the metadata files.

    in summary, it
    -- writes output of the executed function F to a file at path
        opmetadatapath(F) + '/RuntimeOutput.pickle'
    -- appends exit status information to opmetadatapath(F) + '/ExitStatus.csv'
    -- for each file j created by the running of F:
        -- appends file-specific creation information to
        metadatapath(j) + '/CreationRecord.csv'
        -- attaches MetaData returned by the script for file j to
        metadatapath(j) + '/AssociatedMetaData.pickle'

    '''

    MakeAutomaticMetaData(opname,usedefault=True)

    for j in Creates:
        if not PathExists(metadatapath(j)):
            MakeDirs(metadatapath(j))
    if not PathExists(opmetadatapath(opname)):
        MakeDirs(opmetadatapath(opname))

    rtime = str(After - Before)

    strongcopy(opmetadatapath(opname) + '/MostRecentRunPrintout.txt',opmetadatapath(opname) + '/PreviousRunPrintout.txt')
    delete(opmetadatapath(opname) + '/MostRecentRunPrintout.txt')
    strongcopy(TempSOIS,opmetadatapath(opname) + '/MostRecentRunPrintout.txt')

    for j in Creates:
        CRFName = metadatapath(j) + '/CreationRecord.csv'
        if not PathExists(CRFName):
            F = open(CRFName,'w')
            F.write('FileName,Operation,ExitType,TimeStamp,OriginalTimeStamp,Runtime,Diff\n')
            F.close()

        F = open(CRFName,'a')
        F.write(','.join([j,opname,ExitType,str(os.path.getmtime(j) if PathExists(j) else nan),str(OriginalTimes[j]),rtime,str(int(IsDifferent[j]))]) + '\n')
        F.close()

    for j in list(OriginalDirInfo.keys()):
        CRFName = metadatapath(j) + '/CreationRecord.csv'
        Ptime = FindPtime(j)
        if PathExists(CRFName):
            F = open(CRFName,'a')
            F.write(','.join([j,opname,'FromBelow',str(os.path.getmtime(j) if PathExists(j) else nan),str(OriginalDirInfo[j][0]),'0.0','1']) + '\n')
            F.close()

    F = open(opmetadatapath(opname) + '/RuntimeOutput.pickle','wb')
    pickle.dump(RunOutput,F)
    F.close()

    ESFName = opmetadatapath(opname) + '/ExitStatus.csv'
    if not PathExists(ESFName):
        F = open(ESFName,'w')
        F.write('OperationName,ExitType,ExitStatus,TimeStamp,CreateList,CreateTimeStamps,Before,After,Runtime\n')
        F.close()
    CreateString = '\t'.join(Creates)
    CreateTimesList = [os.path.getmtime(l) if PathExists(l) else numpy.nan for l in Creates]
    CreateTimesString = '\t'.join([str(x) for x in CreateTimesList])
    TS = str(max(Before,max(CreateTimesList)))
    NewESFData = ','.join([opname,ExitType, str(ExitStatus),TS , CreateString , CreateTimesString,str(Before),str(After),rtime]) + '\n'
    esf = open(ESFName,'a')
    esf.write(NewESFData)
    esf.close()

    if ExitType == 'Success':
        Written = []
        if isinstance(RunOutput,dict) and 'MetaData' in list(RunOutput.keys()) and isinstance(RunOutput['MetaData'],dict):
            for j in list(RunOutput['MetaData'].keys()):
                if isinstance(j,str):
                    if any([PathAlong(j,k) for k in Creates]) or WORKING_DE.protection != 'ON':
                        if not PathExists(metadatapath(j)):
                            MakeDirs(metadatapath(j))
                        mdp = metadatapath(j) + '/AssociatedMetaData.pickle'

                        if PathExists(mdp):
                            oldmetadata = pickle.load(open(mdp,'rb'))
                        else:
                            oldmetadata = None
                        newmetadata = RunOutput['MetaData'][j]
                        if newmetadata != oldmetadata:
                            metadatafile = open(mdp,'w')
                            pickle.dump(newmetadata,metadatafile)
                            metadatafile.close()
                        print('\nMetaData for ', j, 'written ...')
                        Written.append(mdp)
                        ProcessMetaData(metadatapath(j),objname=j,extensions=['Associated'])

                    else:
                        print('Attempt to write runtime metadata for', j,  ' blocked because Protection is ON and ', j , ' is not contained in any of the declared creates: ', Creates, '.')
                else:
                    print('Attempt to write runtime metadata for', j, ' failed because', j ,'does not appear to be a path string.')
        else:
            print('No runtime metadata output for ', opname , ', or metadata argument in wrong format.')

        BadSet = ListUnion([[kk for kk in RecursiveFileList(metadatapath(k)) if (kk.endswith('AssociatedMetaData.pickle')) and kk not in Written] for k in Creates])
        for kk in BadSet:
            'Deleting', kk, 'as it appears to old irrelevant associated metadata.'
            delete(kk)

    if ExitType == 'Success':
        for j in Creates:
            if IsDifferent[j] and PathExists(j):
                MakeAutomaticMetaData(j)

def MakeAutomaticMetaData(objname,usedefault=False,forced=False,
                                   creates = WORKING_DE.relative_metadata_dir,**kwargs):
    print('Generating automatic metadata for ', objname, '...')
    try:
        SF = __import__(static.LOCAL_SETUP_MODULE)
        imp.reload(SF)
    except:
        traceback.print_exc()
        print('Failed to import SetupFunctions module.  Using default automatic metadata module.')
        X = DEFAULT_GenerateAutomaticMetaData(objname)
    else:
        try:
            metadatafn = getattr(SF,static.LOCAL_METADATA_GENERATOR)
            X = metadatafn(objname,forced=forced,**kwargs)
        except:
            print('Automatic generation of metadata for', objname, 'by user-defined function failed (using default instead).  Here is the error:')
            traceback.print_exc()
            X = DEFAULT_GenerateAutomaticMetaData(objname)

    metapath = opmetadatapath(objname) if IsDotPath(objname) else metadatapath(objname)

    if isinstance(X,dict):
        for j in list(X.keys()):
            metapathj = opmetadatapath(j) if IsDotPath(j) else metadatapath(j)
            if PathAlong(metapathj,metapath) or (IsDotPath(j) and metapath == metapathj):
                if not PathExists(metapathj):
                    MakeDirs(metapathj)
                F = open(metapathj + '/AutomaticMetaData.pickle','wb')
                pickle.dump(X[j],F)
                F.close()

                ProcessMetaData(metapathj,j,extensions=['Automatic'])
        print('... done generating automatic metadata for ', objname, '.')
    else:
        print('Automatically generated metadata for', objname, 'is not in proper format.')

def DEFAULT_GenerateAutomaticMetaData(objname):

    D = {}

    if IsPythonFile(objname) or IsDotPath(objname):
        D['Verbose'] = StoredDocstring(objname)

    if D:
        return  {objname : D}
    else:
        return {}

def ProcessMetaData(metapath,objname=None, extensions = None, usedefault=False,
                                         depends_on = WORKING_DE.relative_metadata_dir):
    if objname is None:
        objname = metapath

    try:
        SF = __import__(static.LOCAL_SETUP_MODULE)
        imp.reload(SF)
    except :
        print('Failed to import SetupFunction module.  Using Default MetaData Processor.')
        DEFAULT_MetaDataProcessor(metapath,objname=objname,extensions=extensions)
    else:
        try:
            metadata_processor = getattr(SF,static.LOCAL_METADATA_PROCESSOR)
            # module has no attr process_metadata.
            # ignored since DEFAULT_MetaDataProcessor called anyway
            metadata_processor(metapath,objname=objname,extensions=extensions)
        except:
            print('Processing of metadata for', objname, 'by user-defined Processor function failed  (using default instead).  Here is the error:')
            traceback.print_exc()
            DEFAULT_MetaDataProcessor(metapath,objname=objname,extensions=extensions)

def DEFAULT_MetaDataProcessor(metapath,objname = None, extensions = None):
    if objname is None:
        objname = metapath

    X = ConsolidateSources(metapath)
    ProcessResources(metapath,X,objname)
    image = ChooseImage(metapath)
    if image:
        X['image'] = image
    F = open(metapath + '/ProcessedMetaData.pickle','wb')
    pickle.dump(X,F)

    text = SummarizeMetaData(X)
    F = open(metapath + '/MetaDataSummary.html','w')
    F.write(text)
    F.close()

def ConsolidateSources(metapath,objname=None,extensions = None):

    consolidated = CombineSources(metapath,keys = ['Resources','author','keywords','signature','title','description','Verbose'],extensions=extensions)

    if 'Resources' in consolidated:
        consolidated['Resources'] = uniqify(ListUnion(list(consolidated['Resources'].values())))

    if 'author' in list(consolidated.keys()):
        consolidated['author'] = '; '.join(list(consolidated['author'].values()))

    if 'title' in list(consolidated.keys()):
        consolidated['title'] = '; '.join(list(consolidated['title'].values()))

    if 'description' in list(consolidated.keys()):
        descrs = list(consolidated['description'].items())
        if len(descrs) == 1:
            consolidated['description'] = descrs[0][1]
        else:
            consolidated['description'] = '\n\n'.join([e + ': ' + d for (e,d) in descrs])

    elif 'Verbose' in list(consolidated.keys()):
        descrs = list(consolidated['Verbose'].items())
        if len(descrs) == 1:
            consolidated['description'] = descrs[0][1]
        else:
            consolidated['description'] = '\n\n'.join([e + ': ' + d for (e,d) in descrs])

    if 'keywords' in list(consolidated.keys()):
        for k in list(consolidated['keywords'].keys()):
            if not is_string_like(consolidated['keywords'][k]):
                consolidated['keywords'][k] = ','.join(consolidated['keywords'][k])

        consolidated['keywords'] = [x.strip() for x in uniqify((','.join(list(consolidated['keywords'].values()))).split(','))]

    if 'signature' in list(consolidated.keys()):
        s = uniqify(list(consolidated['signature'].values()))
        if len(s) == 1:
            consolidated['signature'] = s[0]
        else:
            consolidated['signature'] = ''

    return consolidated

def SummarizeMetaData(X):

    if 'image' in list(X.keys()):
        image = '<img src="' + X['image'] + '"/><br/>'
    else:
        image = ''

    if 'description' in list(X.keys()):
        description = '<strong>Description: </strong>' + X['description'].replace('\n','<br/>')
    else:
        description = ''

    if 'author' in list(X.keys()):
        author = '<strong>Author: </strong>' + X['author']
    else:
        author = ''

    if 'title' in list(X.keys()):
        title = '<strong>Title: </strong>' + X['title']
    else:
        title = ''

    if 'keywords' in list(X.keys()):
        keywords = '<strong>Keywords: </strong>' + ','.join(X['keywords'])
    else:
        keywords = ''

    if 'signature' in list(X.keys()):
        signature = '<strong>Signature: </strong> This appears to be a ' + X['signature'] + ' file.'
    else:
        signature = ''

    text = '<br/>'.join([x for x in [image,title,author,signature,description,keywords] if x != ''])

    return text

def ChooseImage(metapath):
    images = [metapath[2:] + '/' + l for l in listdir(metapath) if 'image' in l.lower() and l.lower().endswith(('.png','.pdf','.gif','.jpg','.tiff','.bmp'))]
    if len(images) > 0:
        return images[0]

def CombineSources(metapath,keys=None,extensions=None,depends_on = WORKING_DE.relative_metadata_dir):

    if extensions is None:
        extensions = ['Attached','Associated','Automatic']

    consolidated = {}
    for ext in extensions:
        fileext = '/' + ext + 'MetaData.pickle'
        if PathExists(metapath + fileext):
            metadata = pickle.load(open(metapath + fileext,'rb'))
            if is_string_like(metadata):
                metadata = {'description' : metadata}

            for k in list(metadata.keys()):
                if keys is None or k in keys:
                    if k not in consolidated:
                        consolidated[k] = {ext : metadata[k] }
                    else:
                        consolidated[k].update({ext : metadata[k]})

    return consolidated

def ProcessResources(metapath,metadata,objname,depends_on=WORKING_DE.relative_root_dir):
    if isinstance(metadata,dict) and 'Resources' in list(metadata.keys()):
        Resources = metadata['Resources']
        if Resources:
            for (source,name) in Resources:
                if is_file_name(name):
                    mdp = os.path.join(metapath,name)
                    if is_external_url(source):
                        E = os.system('wget ' + source + ' -O ' + mdp)
                        if E != 0 or not PathExists(mdp):
                            print('Error attaching metadata to', objname, ': appears to have been unable to locate download URL', source)
                    else:
                        if is_string_like(source) and PathExists(source):
                            try:
                                strongcopy(source,mdp)
                            except:
                                print('Error processing resource metadata to', objname, ': source name',source, 'does not describe a path on the system or a URL.')
                        else:
                            print('Error processing resource metadata to', objname, ': source name',source, 'does not describe a path on the system or a URL.')
                else:
                    print('Error processing resource metadata to', objname, ': metadata name', name, 'isn\'t a file name')

def tabularmetadataforms(pathlist,depends_on = WORKING_DE.relative_metadata_dir):
    attlist = ['description','author','title','keywords']
    recs1 = []
    recs2 = []
    for x in pathlist:
        mdp = metadatapath(x) + '/ProcessedMetaData.pickle'
        if PathExists(mdp):
            M = pickle.load(open(mdp, 'rb'))
            D = {}
            for att in attlist:
                if att in list(M.keys()):
                    D[att] = M[att]
                else:
                    D[att] = ''
            recs1.append((x,) + tuple([D[att].replace('\n',' ') for att in attlist]))

            colnames = M['colnames']
            if 'coldescrs' in list(M.keys()):
                coldescrs = [M['coldescrs'][m] if m in list(M['coldescrs'].keys()) else ''  for m in colnames]
            else:
                coldescrs = ['']*len(colnames)

            recs2 += list(zip([x]*len(colnames),colnames,coldescrs))

    X = tb.tab.tabarray(records = recs1,names=['Path'] + attlist)
    Y = tb.tab.tabarray(records = recs2,names = ['Path','ColName','ColDescr'])

    return [X,Y]

def copymetadata(path,to,depends_on = WORKING_DE.relative_metadata_dir):
    strongcopy(os.path.join(metadatapath(path), 'ProcessedMetaData.pickle'),to)

def loadmetadata(path,depends_on = WORKING_DE.relative_metadata_dir):
    processed_file = os.path.join(metadatapath(path),'ProcessedMetaData.pickle')
    print(processed_file)
    if PathExists(processed_file):
        return pickle.load(open(processed_file,'rb'))

def IsFailure(Path):
    '''
    Returns Boolean True if Path represents is the python dot-path
    of an operation whose most recent run by the autmatic updater
    was a failure.
    '''
    metapath = os.path.join(opmetadatapath(Path), "ExitStatus.csv")
    if PathExists(metapath):
        try:
            print("tried")
            ESD = tb.tab.tabarray(SVfile = metapath,delimiter = ',', lineterminator='\n')
            print(ESD)
            ESD.sort(order = ['TimeStamp'])
            return ESD['ExitType'][-1] == 'Failure'
        except:
            return False
    else:
        return False

def LastTimeChanged(path):
    '''
    Returns last time, according to runtime meta data, that a file (at "path")
    was actually modified (e.g. not simply overwritten, but actually modified.)
    '''

    actualmodtime = os.path.getmtime(path)
    if actualmodtime == FindPtime(path):
        try:
            Data = tb.tab.tabarray(SVfile = metapath,delimiter = ',', lineterminator='\n')
            if len(Data) > 0:
                Data.sort(order=['TimeStamp'])
                Diffs = Data['Diff'].nonzero()[0]
                if len(Diffs) > 0:
                    result = Data['TimeStamp'][Diffs[-1]]
                else:
                    #return actualmodtime
                    result = actualmodtime
            else:
                result = actualmodtime
        except:
            result = actualmodtime
    else:
        result = actualmodtime

    result = datetime.fromtimestamp(result)
    return result

def FindPtime(target,Simple=False):
    '''
    Returns last time, according to runtime meta data, that
    a target was succesfully created, if it is created data.
    '''

    metapath = metadatapath(target) + '/CreationRecord.csv'
    if PathExists(metapath):
        try:
            Data = tb.tab.tabarray(SVfile = metapath,delimiter = ',', lineterminator='\n')
            if len(Data) > 0:
                Data.sort(order=['TimeStamp'])
                if any(Data['ExitType'] == 'Success'):
                    MostRecentSuccess = Data[Data['ExitType'] == 'Success']['TimeStamp'][-1]
                    MoreRecentFailures = Data[(Data['ExitType'] == 'Failure') & (Data['TimeStamp'] > MostRecentSuccess)]
                    if len(MoreRecentFailures) > 0:
                        LeastRecentFailure = MoreRecentFailures['TimeStamp'][0]
                    else:
                        LeastRecentFailure = numpy.inf
                    return Data[(Data['TimeStamp'] >= MostRecentSuccess) & (Data['TimeStamp'] < LeastRecentFailure)]['TimeStamp'][-1]
                else:
                    return numpy.nan
            else:
                return numpy.nan
        except:
            return numpy.nan
        else: pass
    else:
        return numpy.nan

def metadatapath(datapath):
    return os.path.join(WORKING_DE.metadata_dir, datapath.strip('../'))

def opmetadatapath(oppath):
    return os.path.join(WORKING_DE.metadata_dir, oppath.replace('/','', 1))

def main():
    # time = LastTimeChanged("/Users/jen/Desktop/PF/scripts/script.py") #path, not list
    # print(time)

    # new_md = {} # don't know how to format this dictionary, so loading an empty one
    # AttachMetaData(new_md, FileName = "/Users/jen/Desktop/PF/scripts/script.py")

    # copymetadata(f,to)
    # How to call this? hard to format a dictionary from scratch?

    #metadata for a file
    # print(metadatapath("/Users/jen/Desktop/PF/scripts/script.py"))
    # f = loadmetadata("/Users/jen/Desktop/PF/scripts/script.py")
    # print(f) #loads empty dict. check again after make automatic metadata for file

    #metadata for a function, after function called by FullUpdate
    #use dot path instead of normal path!
    # print(metadatapath("/Users.jen.Desktop.PF.scripts.script.manipulate_csv"))
    # f = loadmetadata("/Users.jen.Desktop.PF.scripts.script.manipulate_csv")
    # print(f) #loads {'Verbose': {'Automatic': ''}, 'description': ''}

    # result = IsFailure("/Users.jen.Desktop.PF.scripts.script.manipulate_csv")
    # print(result) #okay for function, not for file

    # MD for file. currently empty dict. key error
    # [X, Y] = tabularmetadataforms(["/Users/jen/Desktop/PF/scripts/script.py"])
    # print(X)
    # print(Y)

    # MD for function. dict exists, but still key error colnames
    [X, Y] = tabularmetadataforms(["/Users.jen.Desktop.PF.scripts.script.manipulate_csv"])
    print(X)
    print(Y)

if __name__ == "__main__":
    main()
